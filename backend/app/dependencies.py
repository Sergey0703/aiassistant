# ====================================
# –§–ê–ô–õ: backend/app/dependencies.py (–û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)
# –ó–∞–º–µ–Ω–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é
# ====================================

"""
–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π LLM
"""

import logging
import sys
import os
import time
import json
import asyncio
from typing import Optional, List, Dict, Any

from app.config import settings

logger = logging.getLogger(__name__)

# ====================================
# –ö–õ–ê–°–°–´-–ó–ê–ì–õ–£–®–ö–ò –î–õ–Ø –ù–ï–î–û–°–¢–£–ü–ù–´–• –°–ï–†–í–ò–°–û–í
# ====================================

class FallbackDocumentService:
    """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è document service –∫–æ–≥–¥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
    
    def __init__(self):
        self.service_type = "fallback"
        # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–∂–∏–¥–∞–µ—Ç –∫–æ–¥
        self.vector_db = type('MockVectorDB', (), {
            'persist_directory': './fallback_db'
        })()
        logger.info("üìù Using fallback document service")
    
    async def search(self, query: str, category: str = None, limit: int = 5):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        logger.warning(f"Fallback search called with query: {query}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        demo_results = [
            {
                "content": f"Demo search result for '{query}'. Install document processing dependencies for real search.",
                "filename": "demo_document.txt",
                "document_id": f"demo_{int(time.time())}",
                "relevance_score": 0.95,
                "metadata": {
                    "status": "demo",
                    "category": category or "general",
                    "service": "fallback"
                }
            }
        ]
        
        return demo_results
    
    async def get_stats(self):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        return {
            "total_documents": 0,
            "categories": ["general", "demo"],
            "database_type": "Fallback Service",
            "error": "Document service not initialized - install dependencies",
            "available_features": [
                "Demo search responses",
                "Basic API structure", 
                "Error handling"
            ],
            "missing_dependencies": [
                "sentence-transformers (for ChromaDB)",
                "ChromaDB or SimpleVectorDB setup"
            ]
        }
    
    async def get_all_documents(self):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        logger.warning("Fallback get_all_documents called")
        return []
    
    async def delete_document(self, doc_id: str):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.warning(f"Fallback delete_document called for ID: {doc_id}")
        return False
    
    async def process_and_store_file(self, file_path: str, category: str = "general"):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞"""
        logger.warning(f"Fallback process_and_store_file called for: {file_path}")
        return False
    
    async def update_document(self, doc_id: str, content: str = None, metadata: Dict = None):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.warning(f"Fallback update_document called for ID: {doc_id}")
        return False

class FallbackScraperService:
    """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è scraper service –∫–æ–≥–¥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
    
    def __init__(self):
        self.service_type = "fallback"
        self.legal_sites_config = {}
        logger.info("üåê Using fallback scraper service")
    
    async def scrape_legal_site(self, url: str):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞"""
        logger.warning(f"Fallback scraper called for URL: {url}")
        
        # –°–æ–∑–¥–∞–µ–º demo –¥–æ–∫—É–º–µ–Ω—Ç
        demo_content = f"""
DEMO: Legal Document from {url}

‚ö†Ô∏è This is a demonstration document. Real web scraping is unavailable.

To enable real scraping, install the required dependencies:
pip install aiohttp beautifulsoup4

üìã Demo Content:
This document would normally contain the actual content from the website.
In real mode, the scraper would extract legal text, articles, and regulations
from the specified URL using advanced HTML parsing techniques.

üîç Scraped from: {url}
üìÖ Demo generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}
üè∑Ô∏è Status: Fallback mode

For full functionality, please install the scraping dependencies.
"""
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
        return type('DemoDocument', (), {
            'url': url,
            'title': f'DEMO: Legal Document from {url}',
            'content': demo_content.strip(),
            'metadata': {
                'status': 'demo',
                'real_scraping': False,
                'scraped_at': time.time(),
                'service': 'fallback',
                'url': url,
                'demo_version': '2.0'
            },
            'category': 'demo'
        })()
    
    async def scrape_multiple_urls(self, urls: List[str], delay: float = 1.0):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö URL"""
        logger.warning(f"Fallback bulk scraper called for {len(urls)} URLs")
        
        results = []
        for i, url in enumerate(urls):
            if i > 0 and delay > 0:
                await asyncio.sleep(delay)
            
            doc = await self.scrape_legal_site(url)
            results.append(doc)
        
        return results

class FallbackLLMService:
    """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è LLM service –∫–æ–≥–¥–∞ Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
    
    def __init__(self):
        self.service_type = "fallback"
        self.ollama_available = False
        logger.info("ü§ñ Using fallback LLM service")
    
    async def answer_legal_question(self, question: str, context_documents: List[Dict], language: str = "en"):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã"""
        logger.warning(f"Fallback LLM called for question: {question[:50]}...")
        
        # –°–æ–∑–¥–∞–µ–º –¥–µ–º–æ –æ—Ç–≤–µ—Ç
        from services.llm_service import LLMResponse
        
        if language == "uk":
            demo_content = f"""‚ö†Ô∏è –î–ï–ú–û –†–ï–ñ–ò–ú: Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π
            
–ù–∞ –æ—Å–Ω–æ–≤—ñ –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —è –±–∏ –≤—ñ–¥–ø–æ–≤—ñ–≤ –Ω–∞ –≤–∞—à–µ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è: "{question}"

üìö –ó–Ω–∞–π–¥–µ–Ω–æ {len(context_documents)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –±–∞–∑—ñ –∑–Ω–∞–Ω—å.

üí° –î–ª—è –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –ø–æ–≤–Ω–æ—Ü—ñ–Ω–Ω–∏—Ö AI-–≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π:
1. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å Ollama: https://ollama.ai
2. –ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ –º–æ–¥–µ–ª—å: ollama pull llama3.2
3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç—ñ—Ç—å —Å–µ—Ä–≤–µ—Ä

üîß –ü–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞—Ç—É—Å: Ollama —Å–µ—Ä–≤—ñ—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π –Ω–∞ http://localhost:11434"""
        else:
            demo_content = f"""‚ö†Ô∏è DEMO MODE: Ollama unavailable
            
Based on the found documents, I would answer your question: "{question}"

üìö Found {len(context_documents)} relevant documents in the knowledge base.

üí° To get full AI responses:
1. Install Ollama: https://ollama.ai
2. Pull a model: ollama pull llama3.2
3. Restart the server

üîß Current status: Ollama service unavailable at http://localhost:11434"""
        
        return LLMResponse(
            content=demo_content,
            model="fallback",
            tokens_used=0,
            response_time=0.1,
            success=False,
            error="Ollama service not available"
        )
    
    async def get_service_status(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å fallback LLM —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            "ollama_available": False,
            "models_available": [],
            "default_model": "fallback",
            "base_url": "N/A",
            "system_prompts_loaded": 0,
            "supported_languages": ["en", "uk"],
            "error": "Ollama service not available - install Ollama and restart server",
            "service_type": "fallback",
            "recommendations": [
                "Install Ollama from https://ollama.ai",
                "Run: ollama pull llama3.2",
                "Ensure Ollama is running on http://localhost:11434",
                "Restart the Legal Assistant server"
            ]
        }

# ====================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï –°–ï–†–í–ò–°–û–í
# ====================================

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
document_service: Optional[object] = None
scraper: Optional[object] = None
llm_service: Optional[object] = None  # –ù–û–í–´–ô –°–ï–†–í–ò–°
SERVICES_AVAILABLE: bool = False
CHROMADB_ENABLED: bool = False
LLM_ENABLED: bool = False  # –ù–û–í–´–ô –§–õ–ê–ì

async def init_services():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –≤–∫–ª—é—á–∞—è LLM"""
    global document_service, scraper, llm_service, SERVICES_AVAILABLE, CHROMADB_ENABLED, LLM_ENABLED
    
    logger.info("üîß Initializing services...")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –ø–∞–ø–∫—É –≤ Python path
    current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if current_dir not in sys.path:
        sys.path.append(current_dir)
    
    # ====================================
    # –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–ï–†–í–ò–°–ê –î–û–ö–£–ú–ï–ù–¢–û–í
    # ====================================
    try:
        if settings.USE_CHROMADB:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ChromaDB
            try:
                from services.chroma_service import DocumentService
                document_service = DocumentService(settings.CHROMADB_PATH)
                CHROMADB_ENABLED = True
                logger.info("‚úÖ ChromaDB service initialized")
            except ImportError as e:
                logger.warning(f"ChromaDB not available, falling back to SimpleVectorDB: {e}")
                try:
                    from services.document_processor import DocumentService
                    document_service = DocumentService(settings.SIMPLE_DB_PATH)
                    CHROMADB_ENABLED = False
                    logger.info("‚úÖ SimpleVectorDB service initialized")
                except ImportError as e2:
                    logger.error(f"SimpleVectorDB also not available: {e2}")
                    document_service = None
        else:
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º SimpleVectorDB
            try:
                from services.document_processor import DocumentService
                document_service = DocumentService(settings.SIMPLE_DB_PATH)
                CHROMADB_ENABLED = False
                logger.info("‚úÖ SimpleVectorDB service initialized (forced)")
            except ImportError as e:
                logger.error(f"SimpleVectorDB not available: {e}")
                document_service = None
        
        if document_service:
            SERVICES_AVAILABLE = True
        
    except Exception as e:
        logger.error(f"‚ùå Error initializing document service: {e}")
        document_service = None
        SERVICES_AVAILABLE = False
        CHROMADB_ENABLED = False
    
    # ====================================
    # –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–ï–†–í–ò–°–ê –ü–ê–†–°–ò–ù–ì–ê
    # ====================================
    try:
        from services.scraper_service import LegalSiteScraper
        scraper = LegalSiteScraper()
        logger.info("‚úÖ Web scraper service initialized")
    except Exception as e:
        logger.error(f"‚ùå Error initializing scraper service: {e}")
        scraper = None
    
    # ====================================
    # –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø LLM –°–ï–†–í–ò–°–ê
    # ====================================
    try:
        if settings.OLLAMA_ENABLED and not settings.LLM_DEMO_MODE:
            from services.llm_service import create_llm_service
            
            llm_service = create_llm_service(
                ollama_url=settings.OLLAMA_BASE_URL,
                model=settings.OLLAMA_DEFAULT_MODEL
            )
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
            status = await llm_service.get_service_status()
            
            if status["ollama_available"]:
                LLM_ENABLED = True
                logger.info("‚úÖ LLM service initialized with Ollama")
                logger.info(f"   Available models: {status['models_available']}")
            else:
                logger.warning(f"‚ö†Ô∏è LLM service created but Ollama unavailable: {status.get('error')}")
                LLM_ENABLED = False
        else:
            logger.info("‚ÑπÔ∏è LLM service disabled in configuration or demo mode")
            LLM_ENABLED = False
            
    except ImportError as e:
        logger.error(f"‚ùå LLM service import failed: {e}")
        llm_service = None
        LLM_ENABLED = False
    except Exception as e:
        logger.error(f"‚ùå Error initializing LLM service: {e}")
        llm_service = None
        LLM_ENABLED = False
    
    # ====================================
    # –§–ò–ù–ê–õ–¨–ù–´–ô –°–¢–ê–¢–£–°
    # ====================================
    logger.info(f"üìä Services status:")
    logger.info(f"   Document service: {'‚úÖ' if document_service else '‚ùå'}")
    logger.info(f"   ChromaDB enabled: {'‚úÖ' if CHROMADB_ENABLED else '‚ùå'}")
    logger.info(f"   Scraper service: {'‚úÖ' if scraper else '‚ùå'}")
    logger.info(f"   LLM service: {'‚úÖ' if LLM_ENABLED else '‚ùå'}")
    logger.info(f"   Overall available: {'‚úÖ' if SERVICES_AVAILABLE else '‚ùå'}")

# ====================================
# DEPENDENCY FUNCTIONS
# ====================================

def get_document_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    if not document_service:
        # –í–º–µ—Å—Ç–æ RuntimeError —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
        logger.debug("Using fallback document service")
        return FallbackDocumentService()
    return document_service

def get_scraper_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    if not scraper:
        # –í–º–µ—Å—Ç–æ RuntimeError —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
        logger.debug("Using fallback scraper service") 
        return FallbackScraperService()
    return scraper

def get_llm_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è LLM —Å–µ—Ä–≤–∏—Å–∞"""
    if not llm_service or not LLM_ENABLED:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –µ—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        logger.debug("Using fallback LLM service")
        return FallbackLLMService()
    return llm_service

def get_services_status():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ä–≤–∏—Å–æ–≤"""
    return {
        "document_service_available": document_service is not None,
        "scraper_available": scraper is not None,
        "llm_available": LLM_ENABLED,
        "llm_service_created": llm_service is not None,
        "chromadb_enabled": CHROMADB_ENABLED,
        "services_available": SERVICES_AVAILABLE,
        "fallback_mode": document_service is None or scraper is None or not LLM_ENABLED,
        "ollama_enabled": settings.OLLAMA_ENABLED,
        "llm_demo_mode": settings.LLM_DEMO_MODE
    }

# ====================================
# –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï UTILITY FUNCTIONS
# ====================================

async def get_system_health():
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–¥–æ—Ä–æ–≤—å–µ —Å–∏—Å—Ç–µ–º—ã"""
    status = get_services_status()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å LLM –µ—Å–ª–∏ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω
    llm_status = {}
    if llm_service and LLM_ENABLED:
        try:
            llm_status = await llm_service.get_service_status()
        except Exception as e:
            llm_status = {"error": str(e), "available": False}
    
    health_info = {
        "overall_status": "healthy" if status["services_available"] and status["llm_available"] else "degraded",
        "services": status,
        "llm_status": llm_status,
        "dependencies": {
            "fastapi": True,  # –ï—Å–ª–∏ –º—ã –¥–æ—à–ª–∏ –¥–æ —Å—é–¥–∞, FastAPI —Ä–∞–±–æ—Ç–∞–µ—Ç
            "pydantic": True, # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è Pydantic
        },
        "features": {
            "document_processing": status["document_service_available"],
            "web_scraping": status["scraper_available"], 
            "vector_search": status["chromadb_enabled"],
            "ai_responses": status["llm_available"],
            "demo_mode": not status["services_available"] or status["llm_demo_mode"]
        }
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    optional_deps = {
        "sentence_transformers": False,
        "aiohttp": False,
        "beautifulsoup4": False,
        "chromadb": False
    }
    
    for dep in optional_deps:
        try:
            __import__(dep)
            optional_deps[dep] = True
        except ImportError:
            pass
    
    health_info["dependencies"].update(optional_deps)
    
    return health_info

async def get_service_recommendations():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–µ—Ä–≤–∏—Å–æ–≤"""
    status = get_services_status()
    recommendations = []
    
    if not status["document_service_available"]:
        recommendations.append({
            "priority": "high",
            "category": "document_processing",
            "message": "Install sentence-transformers for full document processing",
            "command": "pip install sentence-transformers"
        })
    
    if not status["scraper_available"]:
        recommendations.append({
            "priority": "medium", 
            "category": "web_scraping",
            "message": "Install web scraping dependencies for real website parsing",
            "command": "pip install aiohttp beautifulsoup4"
        })
    
    if not status["llm_available"]:
        if not settings.OLLAMA_ENABLED:
            recommendations.append({
                "priority": "high",
                "category": "ai_responses",
                "message": "Enable Ollama in configuration",
                "command": "Set OLLAMA_ENABLED=true in environment or config"
            })
        else:
            recommendations.append({
                "priority": "high",
                "category": "ai_responses", 
                "message": "Install and start Ollama for AI responses",
                "command": "Install from https://ollama.ai, then run: ollama pull llama3.2"
            })
    
    if not status["chromadb_enabled"] and status["document_service_available"]:
        recommendations.append({
            "priority": "low",
            "category": "performance",
            "message": "ChromaDB provides better vector search performance",
            "command": "pip install chromadb"
        })
    
    if not recommendations:
        recommendations.append({
            "priority": "info",
            "category": "status", 
            "message": "All services are running optimally",
            "command": None
        })
    
    return recommendations

async def cleanup_services():
    """–ü—Ä–∞–≤–∏–ª—å–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã –ø—Ä–∏ –≤—ã–∫–ª—é—á–µ–Ω–∏–∏"""
    global llm_service, scraper
    
    logger.info("üßπ Cleaning up services...")
    
    try:
        if llm_service and hasattr(llm_service, 'close'):
            await llm_service.close()
            logger.info("‚úÖ LLM service closed")
    except Exception as e:
        logger.error(f"Error closing LLM service: {e}")
    
    try:
        if scraper and hasattr(scraper, 'close'):
            await scraper.close()
            logger.info("‚úÖ Scraper service closed")
    except Exception as e:
        logger.error(f"Error closing scraper service: {e}")
    
    logger.info("‚úÖ Services cleanup completed")

def create_fallback_response(service_name: str, operation: str, **kwargs):
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π fallback –æ—Ç–≤–µ—Ç"""
    return {
        "service": service_name,
        "operation": operation,
        "status": "fallback",
        "message": f"{service_name} is running in demo mode",
        "data": kwargs.get("data", {}),
        "recommendations": [
            f"Install required dependencies for full {service_name} functionality"
        ],
        "timestamp": time.time()
    }

# ====================================
# –≠–ö–°–ü–û–†–¢
# ====================================

__all__ = [
    "init_services",
    "cleanup_services",  # –ù–û–í–´–ô –≠–ö–°–ü–û–†–¢
    "get_document_service", 
    "get_scraper_service",
    "get_llm_service",  # –ù–û–í–´–ô –≠–ö–°–ü–û–†–¢
    "get_services_status",
    "get_system_health",
    "get_service_recommendations",
    "FallbackDocumentService",
    "FallbackScraperService",
    "FallbackLLMService",  # –ù–û–í–´–ô –≠–ö–°–ü–û–†–¢
    "SERVICES_AVAILABLE",
    "CHROMADB_ENABLED",
    "LLM_ENABLED",  # –ù–û–í–´–ô –≠–ö–°–ü–û–†–¢
    "document_service",
    "scraper",
    "llm_service"  # –ù–û–í–´–ô –≠–ö–°–ü–û–†–¢
]