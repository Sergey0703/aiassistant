# ====================================
# –§–ê–ô–õ: backend/app/dependencies.py (–ü–û–õ–ù–ê–Ø –í–ï–†–°–ò–Ø)
# –ó–∞–º–µ–Ω–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é
# ====================================

"""
–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
"""

import logging
import sys
import os
import time
import json
import asyncio
from typing import Optional, List, Dict, Any

from app.config import settings

logger = logging.getLogger(__name__)

# ====================================
# –ö–õ–ê–°–°–´-–ó–ê–ì–õ–£–®–ö–ò –î–õ–Ø –ù–ï–î–û–°–¢–£–ü–ù–´–• –°–ï–†–í–ò–°–û–í
# ====================================

class FallbackDocumentService:
    """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è document service –∫–æ–≥–¥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
    
    def __init__(self):
        self.service_type = "fallback"
        # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –æ–∂–∏–¥–∞–µ—Ç –∫–æ–¥
        self.vector_db = type('MockVectorDB', (), {
            'persist_directory': './fallback_db'
        })()
        logger.info("üìù Using fallback document service")
    
    async def search(self, query: str, category: str = None, limit: int = 5):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        logger.warning(f"Fallback search called with query: {query}")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        demo_results = [
            {
                "content": f"Demo search result for '{query}'. Install document processing dependencies for real search.",
                "filename": "demo_document.txt",
                "document_id": f"demo_{int(time.time())}",
                "relevance_score": 0.95,
                "metadata": {
                    "status": "demo",
                    "category": category or "general",
                    "service": "fallback"
                }
            }
        ]
        
        return demo_results
    
    async def get_stats(self):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        return {
            "total_documents": 0,
            "categories": ["general", "demo"],
            "database_type": "Fallback Service",
            "error": "Document service not initialized - install dependencies",
            "available_features": [
                "Demo search responses",
                "Basic API structure", 
                "Error handling"
            ],
            "missing_dependencies": [
                "sentence-transformers (for ChromaDB)",
                "ChromaDB or SimpleVectorDB setup"
            ]
        }
    
    async def get_all_documents(self):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        logger.warning("Fallback get_all_documents called")
        return []
    
    async def delete_document(self, doc_id: str):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.warning(f"Fallback delete_document called for ID: {doc_id}")
        return False
    
    async def process_and_store_file(self, file_path: str, category: str = "general"):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞"""
        logger.warning(f"Fallback process_and_store_file called for: {file_path}")
        return False
    
    async def update_document(self, doc_id: str, content: str = None, metadata: Dict = None):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        logger.warning(f"Fallback update_document called for ID: {doc_id}")
        return False

class FallbackScraperService:
    """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è scraper service –∫–æ–≥–¥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
    
    def __init__(self):
        self.service_type = "fallback"
        self.legal_sites_config = {}
        logger.info("üåê Using fallback scraper service")
    
    async def scrape_legal_site(self, url: str):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞"""
        logger.warning(f"Fallback scraper called for URL: {url}")
        
        # –°–æ–∑–¥–∞–µ–º demo –¥–æ–∫—É–º–µ–Ω—Ç
        demo_content = f"""
DEMO: Legal Document from {url}

‚ö†Ô∏è This is a demonstration document. Real web scraping is unavailable.

To enable real scraping, install the required dependencies:
pip install aiohttp beautifulsoup4

üìã Demo Content:
This document would normally contain the actual content from the website.
In real mode, the scraper would extract legal text, articles, and regulations
from the specified URL using advanced HTML parsing techniques.

üîç Scraped from: {url}
üìÖ Demo generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}
üè∑Ô∏è Status: Fallback mode

For full functionality, please install the scraping dependencies.
"""
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
        return type('DemoDocument', (), {
            'url': url,
            'title': f'DEMO: Legal Document from {url}',
            'content': demo_content.strip(),
            'metadata': {
                'status': 'demo',
                'real_scraping': False,
                'scraped_at': time.time(),
                'service': 'fallback',
                'url': url,
                'demo_version': '2.0'
            },
            'category': 'demo'
        })()
    
    async def scrape_multiple_urls(self, urls: List[str], delay: float = 1.0):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö URL"""
        logger.warning(f"Fallback bulk scraper called for {len(urls)} URLs")
        
        results = []
        for i, url in enumerate(urls):
            if i > 0 and delay > 0:
                await asyncio.sleep(delay)
            
            doc = await self.scrape_legal_site(url)
            results.append(doc)
        
        return results
    
    async def validate_url(self, url: str):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ URL"""
        from urllib.parse import urlparse
        
        try:
            parsed = urlparse(url)
            return {
                "url": url,
                "valid": bool(parsed.scheme and parsed.netloc),
                "reachable": False,  # –í fallback —Ä–µ–∂–∏–º–µ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∞–ª—å–Ω—É—é –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
                "issues": ["Real URL validation unavailable in fallback mode"],
                "warnings": ["Install aiohttp for real URL validation"],
                "service": "fallback"
            }
        except Exception as e:
            return {
                "url": url,
                "valid": False,
                "issues": [f"URL validation error: {e}"],
                "service": "fallback"
            }
    
    def get_stats(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ fallback scraper"""
        return {
            "service_type": "fallback",
            "real_scraping_available": False,
            "demo_mode": True,
            "supported_features": [
                "Demo document generation",
                "URL validation",
                "Bulk processing simulation"
            ],
            "missing_dependencies": [
                "aiohttp",
                "beautifulsoup4"
            ]
        }

# ====================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï –°–ï–†–í–ò–°–û–í
# ====================================

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
document_service: Optional[object] = None
scraper: Optional[object] = None
SERVICES_AVAILABLE: bool = False
CHROMADB_ENABLED: bool = False

async def init_services():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    global document_service, scraper, SERVICES_AVAILABLE, CHROMADB_ENABLED
    
    logger.info("üîß Initializing services...")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –ø–∞–ø–∫—É –≤ Python path
    current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if current_dir not in sys.path:
        sys.path.append(current_dir)
    
    # ====================================
    # –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–ï–†–í–ò–°–ê –î–û–ö–£–ú–ï–ù–¢–û–í
    # ====================================
    try:
        if settings.USE_CHROMADB:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ChromaDB
            try:
                from services.chroma_service import DocumentService
                document_service = DocumentService(settings.CHROMADB_PATH)
                CHROMADB_ENABLED = True
                logger.info("‚úÖ ChromaDB service initialized")
            except ImportError as e:
                logger.warning(f"ChromaDB not available, falling back to SimpleVectorDB: {e}")
                try:
                    from services.document_processor import DocumentService
                    document_service = DocumentService(settings.SIMPLE_DB_PATH)
                    CHROMADB_ENABLED = False
                    logger.info("‚úÖ SimpleVectorDB service initialized")
                except ImportError as e2:
                    logger.error(f"SimpleVectorDB also not available: {e2}")
                    document_service = None
        else:
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º SimpleVectorDB
            try:
                from services.document_processor import DocumentService
                document_service = DocumentService(settings.SIMPLE_DB_PATH)
                CHROMADB_ENABLED = False
                logger.info("‚úÖ SimpleVectorDB service initialized (forced)")
            except ImportError as e:
                logger.error(f"SimpleVectorDB not available: {e}")
                document_service = None
        
        if document_service:
            SERVICES_AVAILABLE = True
        
    except Exception as e:
        logger.error(f"‚ùå Error initializing document service: {e}")
        document_service = None
        SERVICES_AVAILABLE = False
        CHROMADB_ENABLED = False
    
    # ====================================
    # –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–ï–†–í–ò–°–ê –ü–ê–†–°–ò–ù–ì–ê
    # ====================================
    try:
        from services.scraper_service import LegalSiteScraper
        scraper = LegalSiteScraper()
        logger.info("‚úÖ Web scraper service initialized")
    except Exception as e:
        logger.error(f"‚ùå Error initializing scraper service: {e}")
        scraper = None
    
    # ====================================
    # –§–ò–ù–ê–õ–¨–ù–´–ô –°–¢–ê–¢–£–°
    # ====================================
    logger.info(f"üìä Services status:")
    logger.info(f"   Document service: {'‚úÖ' if document_service else '‚ùå'}")
    logger.info(f"   ChromaDB enabled: {'‚úÖ' if CHROMADB_ENABLED else '‚ùå'}")
    logger.info(f"   Scraper service: {'‚úÖ' if scraper else '‚ùå'}")
    logger.info(f"   Overall available: {'‚úÖ' if SERVICES_AVAILABLE else '‚ùå'}")

# ====================================
# DEPENDENCY FUNCTIONS
# ====================================

def get_document_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    if not document_service:
        # –í–º–µ—Å—Ç–æ RuntimeError —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
        logger.debug("Using fallback document service")
        return FallbackDocumentService()
    return document_service

def get_scraper_service():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    if not scraper:
        # –í–º–µ—Å—Ç–æ RuntimeError —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
        logger.debug("Using fallback scraper service") 
        return FallbackScraperService()
    return scraper

def get_services_status():
    """Dependency –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ä–≤–∏—Å–æ–≤"""
    return {
        "document_service_available": document_service is not None,
        "scraper_available": scraper is not None,
        "chromadb_enabled": CHROMADB_ENABLED,
        "services_available": SERVICES_AVAILABLE,
        "fallback_mode": document_service is None or scraper is None
    }

# ====================================
# –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï UTILITY FUNCTIONS
# ====================================

async def get_system_health():
    """–ü–æ–ª—É—á–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–¥–æ—Ä–æ–≤—å–µ —Å–∏—Å—Ç–µ–º—ã"""
    status = get_services_status()
    
    health_info = {
        "overall_status": "healthy" if status["services_available"] else "degraded",
        "services": status,
        "dependencies": {
            "fastapi": True,  # –ï—Å–ª–∏ –º—ã –¥–æ—à–ª–∏ –¥–æ —Å—é–¥–∞, FastAPI —Ä–∞–±–æ—Ç–∞–µ—Ç
            "pydantic": True, # –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è Pydantic
        },
        "features": {
            "document_processing": status["document_service_available"],
            "web_scraping": status["scraper_available"], 
            "vector_search": status["chromadb_enabled"],
            "demo_mode": not status["services_available"]
        }
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    optional_deps = {
        "sentence_transformers": False,
        "aiohttp": False,
        "beautifulsoup4": False,
        "chromadb": False
    }
    
    for dep in optional_deps:
        try:
            __import__(dep)
            optional_deps[dep] = True
        except ImportError:
            pass
    
    health_info["dependencies"].update(optional_deps)
    
    return health_info

async def get_service_recommendations():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é —Å–µ—Ä–≤–∏—Å–æ–≤"""
    status = get_services_status()
    recommendations = []
    
    if not status["document_service_available"]:
        recommendations.append({
            "priority": "high",
            "category": "document_processing",
            "message": "Install sentence-transformers for full document processing",
            "command": "pip install sentence-transformers"
        })
    
    if not status["scraper_available"]:
        recommendations.append({
            "priority": "medium", 
            "category": "web_scraping",
            "message": "Install web scraping dependencies for real website parsing",
            "command": "pip install aiohttp beautifulsoup4"
        })
    
    if not status["chromadb_enabled"] and status["document_service_available"]:
        recommendations.append({
            "priority": "low",
            "category": "performance",
            "message": "ChromaDB provides better vector search performance",
            "command": "pip install chromadb"
        })
    
    if not recommendations:
        recommendations.append({
            "priority": "info",
            "category": "status", 
            "message": "All services are running optimally",
            "command": None
        })
    
    return recommendations

def create_fallback_response(service_name: str, operation: str, **kwargs):
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π fallback –æ—Ç–≤–µ—Ç"""
    return {
        "service": service_name,
        "operation": operation,
        "status": "fallback",
        "message": f"{service_name} is running in demo mode",
        "data": kwargs.get("data", {}),
        "recommendations": [
            f"Install required dependencies for full {service_name} functionality"
        ],
        "timestamp": time.time()
    }

# ====================================
# –≠–ö–°–ü–û–†–¢
# ====================================

__all__ = [
    "init_services",
    "get_document_service", 
    "get_scraper_service",
    "get_services_status",
    "get_system_health",
    "get_service_recommendations",
    "FallbackDocumentService",
    "FallbackScraperService",
    "SERVICES_AVAILABLE",
    "CHROMADB_ENABLED",
    "document_service",
    "scraper"
]