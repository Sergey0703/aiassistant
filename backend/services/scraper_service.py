"""
–†–µ–∞–ª—å–Ω—ã–π web scraper —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü
"""
import asyncio
import time
from dataclasses import dataclass
from typing import List, Dict, Optional
import logging
import re

logger = logging.getLogger(__name__)

@dataclass
class ScrapedDocument:
    url: str
    title: str
    content: str
    metadata: Dict
    category: str = "scraped"

class LegalSiteScraper:
    """–†–µ–∞–ª—å–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML"""
    
    def __init__(self):
        self.legal_sites_config = {
            # –ò—Ä–ª–∞–Ω–¥—Å–∫–∏–µ —Å–∞–π—Ç—ã
            "citizensinformation.ie": {
                "title": "h1, .page-title, .main-title",
                "content": ".main-content, .content, article, .text-content",
                "exclude": "nav, .navigation, footer, .sidebar, .ads, .menu"
            },
            "irishstatutebook.ie": {
                "title": "h1, .title",
                "content": ".akn-akomaNtoso, .content, .main",
                "exclude": ".navigation, .sidebar"
            },
            "courts.ie": {
                "title": "h1, .page-title",
                "content": ".content, .main, article",
                "exclude": ".header, .footer, .navigation"
            },
            # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ —Å–∞–π—Ç—ã
            "zakon.rada.gov.ua": {
                "title": "h1, .page-title",
                "content": ".field-item, .content, .main",
                "exclude": ".sidebar, .navigation, .menu"
            },
            "court.gov.ua": {
                "title": "h1",
                "content": ".content-area, .main-content",
                "exclude": ".header, .footer"
            }
        }
    
    async def scrape_legal_site(self, url: str) -> Optional[ScrapedDocument]:
        """–ü–∞—Ä—Å–∏—Ç –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            try:
                import requests
                from bs4 import BeautifulSoup
                from urllib.parse import urlparse
            except ImportError:
                logger.warning("requests –∏–ª–∏ beautifulsoup4 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ–º–æ —Ä–µ–∂–∏–º")
                return await self._create_demo_document(url)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            logger.info(f"Fetching: {url}")
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –¥–æ–º–µ–Ω–∞
            domain = urlparse(url).netloc
            selectors = self.legal_sites_config.get(domain, {})
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = self._extract_title(soup, selectors) or f"–î–æ–∫—É–º–µ–Ω—Ç —Å {domain}"
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            content = self._extract_content(soup, selectors)
            
            if not content or len(content.strip()) < 100:
                logger.warning(f"–ú–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ–ª—É—á–µ–Ω–æ —Å {url}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ–º–æ")
                return await self._create_demo_document(url)
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                "scraped_at": time.time(),
                "status_code": response.status_code,
                "content_type": response.headers.get('content-type', ''),
                "domain": domain,
                "url": url,
                "word_count": len(content.split()),
                "char_count": len(content),
                "real_scraping": True
            }
            
            logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ {len(content)} —Å–∏–º–≤–æ–ª–æ–≤ —Å {url}")
            
            return ScrapedDocument(
                url=url,
                title=title,
                content=content,
                metadata=metadata,
                category=self._categorize_by_domain(domain)
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {str(e)}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ–º–æ –¥–æ–∫—É–º–µ–Ω—Ç
            return await self._create_demo_document(url)
    
    async def _create_demo_document(self, url: str) -> ScrapedDocument:
        """–°–æ–∑–¥–∞–µ—Ç –¥–µ–º–æ –¥–æ–∫—É–º–µ–Ω—Ç –µ—Å–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è"""
        demo_content = f"""
–î–ï–ú–û: –î–æ–∫—É–º–µ–Ω—Ç —Å {url}

‚ö†Ô∏è –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç. –†–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è –ø–æ –æ–¥–Ω–æ–π –∏–∑ –ø—Ä–∏—á–∏–Ω:
- –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ requests/beautifulsoup4
- –°–∞–π—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –¥–æ—Å—Ç—É–ø
- –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç—å—é
- –ù–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã

–î–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ:
pip install requests beautifulsoup4

–°–æ–¥–µ—Ä–∂–∏–º–æ–µ demo –¥–æ–∫—É–º–µ–Ω—Ç–∞:
–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞—Ö.
–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –≤–∫–ª—é—á–∞—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, —Å—Ä–æ–∫–∞—Ö –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö.

URL: {url}
–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {time.strftime('%Y-%m-%d %H:%M:%S')}
–°—Ç–∞—Ç—É—Å: –î–µ–º–æ —Ä–µ–∂–∏–º
"""
        
        return ScrapedDocument(
            url=url,
            title=f"–î–ï–ú–û: –î–æ–∫—É–º–µ–Ω—Ç —Å {url}",
            content=demo_content.strip(),
            metadata={
                "scraped_at": time.time(),
                "status": "demo",
                "real_scraping": False,
                "word_count": len(demo_content.split()),
                "char_count": len(demo_content),
                "url": url
            },
            category="demo"
        )
    
    def _extract_title(self, soup, selectors: Dict[str, str] = None) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        if selectors and "title" in selectors:
            for selector in selectors["title"].split(", "):
                title_elem = soup.select_one(selector.strip())
                if title_elem and title_elem.get_text(strip=True):
                    return title_elem.get_text(strip=True)
        
        # Fallback —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        for selector in ['h1', 'title', '.title', '.page-title', '.main-title']:
            elem = soup.select_one(selector)
            if elem and elem.get_text(strip=True):
                return elem.get_text(strip=True)
        
        return "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
    
    def _extract_content(self, soup, selectors: Dict[str, str] = None) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for tag in soup(["script", "style", "nav", "header", "footer", "aside", "iframe", "noscript"]):
            tag.decompose()
        
        # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ exclude —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
        if selectors and "exclude" in selectors:
            for selector in selectors["exclude"].split(", "):
                for elem in soup.select(selector.strip()):
                    elem.decompose()
        
        content_text = ""
        
        # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –ø–æ content —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
        if selectors and "content" in selectors:
            for selector in selectors["content"].split(", "):
                content_elem = soup.select_one(selector.strip())
                if content_elem:
                    content_text = content_elem.get_text()
                    break
        
        # Fallback —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        if not content_text:
            for selector in ['.main-content', '.content', 'article', '.post', 'main', '.main']:
                elem = soup.select_one(selector)
                if elem:
                    content_text = elem.get_text()
                    break
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback - –≤–µ—Å—å body
        if not content_text:
            body = soup.find('body')
            if body:
                content_text = body.get_text()
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        return self._clean_text(content_text)
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç"""
        if not text:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'\.{3,}', '...', text)
        text = re.sub(r'-{3,}', '---', text)
        
        # –£–±–∏—Ä–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ (–º–µ–Ω—é, –Ω–∞–≤–∏–≥–∞—Ü–∏—è)
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if len(line) > 10:  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ –¥–ª–∏–Ω–Ω–µ–µ 10 —Å–∏–º–≤–æ–ª–æ–≤
                cleaned_lines.append(line)
        
        result = '\n'.join(cleaned_lines).strip()
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if len(result) > 10000:
            result = result[:10000] + "\n\n[–î–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–æ 10000 —Å–∏–º–≤–æ–ª–æ–≤]"
        
        return result
    
    def _categorize_by_domain(self, domain: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –¥–æ–º–µ–Ω—É"""
        if "zakon" in domain or "law" in domain:
            return "legislation"
        elif "court" in domain:
            return "jurisprudence"  
        elif "justice" in domain or "minjust" in domain:
            return "government"
        elif "citizen" in domain or "immigration" in domain:
            return "civil_rights"
        elif "rada.gov.ua" in domain:
            return "ukraine_legal"
        elif any(irish in domain for irish in ["courts.ie", "irishstatutebook.ie", "citizensinformation.ie"]):
            return "ireland_legal"
        else:
            return "scraped"
    
    async def scrape_multiple_urls(self, urls: List[str], delay: float = 1.0) -> List[ScrapedDocument]:
        """–ü–∞—Ä—Å–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ URL —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π"""
        documents = []
        
        for i, url in enumerate(urls):
            logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ {i+1}/{len(urls)}: {url}")
            
            document = await self.scrape_legal_site(url)
            if document:
                documents.append(document)
                if document.metadata.get("real_scraping", False):
                    logger.info(f"‚úÖ –†–µ–∞–ª—å–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {document.title}")
                else:
                    logger.info(f"‚ö†Ô∏è –î–µ–º–æ –¥–æ–∫—É–º–µ–Ω—Ç: {document.title}")
            else:
                logger.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–∞—Ä—Å–∏—Ç—å: {url}")
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –≤–µ–∂–ª–∏–≤–æ—Å—Ç–∏ –∫ —Å–µ—Ä–≤–µ—Ä–∞–º
            if delay > 0 and i < len(urls) - 1:
                await asyncio.sleep(delay)
        
        return documents

# –ü—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤
UKRAINE_LEGAL_URLS = [
    "https://zakon.rada.gov.ua/laws/main",
    "https://court.gov.ua/",
    "https://minjust.gov.ua/",
    "https://ccu.gov.ua/",
    "https://npu.gov.ua/"
]

IRELAND_LEGAL_URLS = [
    "https://www.irishstatutebook.ie/",
    "https://www.courts.ie/",
    "https://www.citizensinformation.ie/en/",
    "https://www.justice.ie/",
    "https://www.oireachtas.ie/"
]

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
async def test_real_scraper():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥"""
    scraper = LegalSiteScraper()
    
    test_urls = [
        "https://www.citizensinformation.ie/en/moving-country/irish-citizenship/",
        "https://www.example.com"  # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ fallback
    ]
    
    for url in test_urls:
        print(f"\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º: {url}")
        document = await scraper.scrape_legal_site(url)
        
        if document:
            print(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {document.title}")
            print(f"üìä –†–∞–∑–º–µ—Ä: {len(document.content)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"üîç –†–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥: {document.metadata.get('real_scraping', False)}")
            print(f"üìÑ –ü—Ä–µ–≤—å—é: {document.content[:200]}...")
        else:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å")

if __name__ == "__main__":
    asyncio.run(test_real_scraper())