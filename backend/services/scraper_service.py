# ====================================
# –§–ê–ô–õ: backend/services/scraper_service.py (–ü–û–õ–ù–ê–Ø –í–ï–†–°–ò–Ø)
# –ó–∞–º–µ–Ω–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é
# ====================================

"""
–†–µ–∞–ª—å–Ω—ã–π web scraper —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ —Ä–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥, —Ç–∞–∫ –∏ demo —Ä–µ–∂–∏–º
"""

import asyncio
import time
import aiohttp
import ssl
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple
import logging
import re
from urllib.parse import urlparse, urljoin, quote, unquote
from pathlib import Path
import json
import hashlib

logger = logging.getLogger(__name__)

@dataclass
class ScrapedDocument:
    """–ú–æ–¥–µ–ª—å —Å–ø–∞—Ä—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    url: str
    title: str
    content: str
    metadata: Dict
    category: str = "scraped"
    
    def __post_init__(self):
        """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è"""
        # –û—á–∏—â–∞–µ–º –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        self.title = self.title.strip()[:500] if self.title else "Untitled"
        self.content = self._clean_content(self.content)
        self.url = self.url.strip()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        if not self.metadata:
            self.metadata = {}
        
        self.metadata.update({
            "content_length": len(self.content),
            "word_count": len(self.content.split()),
            "title_length": len(self.title),
            "processed_at": time.time()
        })
    
    def _clean_content(self, content: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if not content:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        content = re.sub(r'\s+', ' ', content)
        content = re.sub(r'\n\s*\n', '\n\n', content)
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã
        content = re.sub(r'\.{3,}', '...', content)
        content = re.sub(r'-{3,}', '---', content)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
        if len(content) > 15000:  # 15KB –ª–∏–º–∏—Ç
            content = content[:15000] + "\n\n[–î–æ–∫—É–º–µ–Ω—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–æ 15000 —Å–∏–º–≤–æ–ª–æ–≤]"
        
        return content.strip()

class SiteConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–∞–π—Ç–∞"""
    
    def __init__(self, domain: str, **config):
        self.domain = domain
        self.title_selectors = config.get("title", "h1, title, .title")
        self.content_selectors = config.get("content", ".content, article, main")
        self.exclude_selectors = config.get("exclude", "nav, footer, .sidebar, script, style")
        self.custom_parser = config.get("custom_parser")
        self.encoding = config.get("encoding", "utf-8")
        self.timeout = config.get("timeout", 15)
        self.headers = config.get("headers", {})

class LegalSiteScraper:
    """–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–∫—Ä–∞–ø–µ—Ä –¥–ª—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤"""
    
    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
        self.legal_sites_config = self._initialize_site_configs()
        self.demo_mode = False
        self.stats = {
            "total_requests": 0,
            "successful_scrapes": 0,
            "failed_scrapes": 0,
            "demo_responses": 0,
            "average_response_time": 0.0
        }
    
    def _initialize_site_configs(self) -> Dict[str, SiteConfig]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
        configs = {}
        
        # –ò—Ä–ª–∞–Ω–¥—Å–∫–∏–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Å–∞–π—Ç—ã
        configs["citizensinformation.ie"] = SiteConfig(
            "citizensinformation.ie",
            title="h1, .page-title, .main-title",
            content=".main-content, .content, article, .text-content",
            exclude="nav, .navigation, footer, .sidebar, .ads, .menu, .breadcrumb",
            encoding="utf-8"
        )
        
        configs["irishstatutebook.ie"] = SiteConfig(
            "irishstatutebook.ie",
            title="h1, .title, .act-title",
            content=".akn-akomaNtoso, .content, .main, .act-body",
            exclude=".navigation, .sidebar, .header, .footer"
        )
        
        configs["courts.ie"] = SiteConfig(
            "courts.ie",
            title="h1, .page-title, .judgment-title",
            content=".content, .main, article, .judgment-text",
            exclude=".header, .footer, .navigation, .sidebar"
        )
        
        configs["justice.ie"] = SiteConfig(
            "justice.ie",
            title="h1, .page-title",
            content=".main-content, .content, article",
            exclude=".header, .footer, .navigation, .sidebar, .news-listing"
        )
        
        # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Å–∞–π—Ç—ã
        configs["zakon.rada.gov.ua"] = SiteConfig(
            "zakon.rada.gov.ua",
            title="h1, .page-title, .doc-title",
            content=".field-item, .content, .main, .document-content",
            exclude=".sidebar, .navigation, .menu, .breadcrumb",
            encoding="utf-8"
        )
        
        configs["court.gov.ua"] = SiteConfig(
            "court.gov.ua",
            title="h1, .court-title",
            content=".content-area, .main-content, .court-content",
            exclude=".header, .footer, .navigation"
        )
        
        configs["minjust.gov.ua"] = SiteConfig(
            "minjust.gov.ua",
            title="h1, .page-title",
            content=".main-content, .content, article",
            exclude=".header, .footer, .sidebar, .navigation"
        )
        
        configs["ccu.gov.ua"] = SiteConfig(
            "ccu.gov.ua", 
            title="h1, .title",
            content=".content, .main, .decision-text",
            exclude=".navigation, .sidebar"
        )
        
        # –û–±—â–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö CMS
        configs["wordpress"] = SiteConfig(
            "wordpress",
            title="h1, .entry-title, .post-title",
            content=".entry-content, .post-content, .content",
            exclude=".sidebar, .navigation, .comments"
        )
        
        configs["drupal"] = SiteConfig(
            "drupal",
            title="h1, .page-title",
            content=".field-item, .content, .main",
            exclude=".sidebar, .navigation, .menu"
        )
        
        return configs
    
    async def __aenter__(self):
        """Async context manager entry"""
        await self._ensure_session()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self.close()
    
    async def _ensure_session(self):
        """–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∞–∫—Ç–∏–≤–Ω–æ–π —Å–µ—Å—Å–∏–∏"""
        if not self.session or self.session.closed:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ SSL –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ–±–ª–µ–º —Å —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞–º–∏
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = False
            ssl_context.verify_mode = ssl.CERT_NONE
            
            connector = aiohttp.TCPConnector(
                ssl=ssl_context,
                limit=100,
                limit_per_host=10,
                ttl_dns_cache=300,
                use_dns_cache=True
            )
            
            timeout = aiohttp.ClientTimeout(total=30, connect=10)
            
            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.9,uk;q=0.8',
                    'Accept-Encoding': 'gzip, deflate',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1'
                }
            )
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–µ—Å—Å–∏—é"""
        if self.session and not self.session.closed:
            await self.session.close()
    
    async def scrape_legal_site(self, url: str, custom_config: Optional[Dict] = None) -> Optional[ScrapedDocument]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ —Å–∞–π—Ç–∞
        
        Args:
            url: URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            custom_config: –ö–∞—Å—Ç–æ–º–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            
        Returns:
            ScrapedDocument –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        try:
            logger.info(f"üîç Starting scrape for: {url}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
            if not await self._check_dependencies():
                logger.warning("Missing dependencies, switching to demo mode")
                self.demo_mode = True
                document = await self._create_demo_document(url)
                self.stats["demo_responses"] += 1
                return document
            
            await self._ensure_session()
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Å–∞–π—Ç–∞
            site_config = self._get_site_config(url, custom_config)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º HTTP –∑–∞–ø—Ä–æ—Å
            response_data = await self._fetch_url(url, site_config)
            
            if not response_data:
                logger.warning(f"No response data for {url}, creating demo document")
                return await self._create_demo_document(url)
            
            # –ü–∞—Ä—Å–∏–º HTML
            document = await self._parse_html_content(
                url, 
                response_data["content"], 
                response_data["encoding"],
                site_config,
                response_data.get("response_headers", {})
            )
            
            if document:
                self.stats["successful_scrapes"] += 1
                elapsed = time.time() - start_time
                self._update_response_time(elapsed)
                logger.info(f"‚úÖ Successfully scraped {url} in {elapsed:.2f}s")
                return document
            else:
                logger.warning(f"Failed to parse content from {url}")
                return await self._create_demo_document(url)
                
        except Exception as e:
            self.stats["failed_scrapes"] += 1
            logger.error(f"‚ùå Error scraping {url}: {str(e)}")
            return await self._create_demo_document(url)
        
        finally:
            elapsed = time.time() - start_time
            self._update_response_time(elapsed)
    
    async def _check_dependencies(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫"""
        try:
            import aiohttp
            from bs4 import BeautifulSoup
            return True
        except ImportError as e:
            logger.warning(f"Missing dependencies: {e}")
            return False
    
    def _get_site_config(self, url: str, custom_config: Optional[Dict] = None) -> SiteConfig:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞"""
        domain = urlparse(url).netloc.lower()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –µ—Å–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∞
        if custom_config:
            return SiteConfig(domain, **custom_config)
        
        # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞
        if domain in self.legal_sites_config:
            return self.legal_sites_config[domain]
        
        # –ò—â–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for config_domain, config in self.legal_sites_config.items():
            if config_domain in domain or domain in config_domain:
                return config
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º CMS –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        cms_type = self._detect_cms_type(url)
        if cms_type and cms_type in self.legal_sites_config:
            return self.legal_sites_config[cms_type]
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        return SiteConfig(
            domain,
            title="h1, title, .title, .page-title",
            content=".content, .main, article, .post, .entry-content",
            exclude="nav, footer, .sidebar, .navigation, .menu, script, style, .ads"
        )
    
    def _detect_cms_type(self, url: str) -> Optional[str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø CMS –ø–æ URL –∏–ª–∏ –¥—Ä—É–≥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è CMS
        if "wp-content" in url or "wordpress" in url:
            return "wordpress"
        elif "sites/default" in url or "drupal" in url:
            return "drupal"
        return None
    
    async def _fetch_url(self, url: str, site_config: SiteConfig) -> Optional[Dict]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç HTTP –∑–∞–ø—Ä–æ—Å –∫ URL"""
        try:
            headers = {
                **self.session.headers,
                **site_config.headers
            }
            
            async with self.session.get(
                url, 
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=site_config.timeout)
            ) as response:
                
                logger.info(f"üì° HTTP {response.status} for {url}")
                
                if response.status != 200:
                    logger.warning(f"Non-200 status code: {response.status}")
                    return None
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
                encoding = self._detect_encoding(response, site_config.encoding)
                
                # –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                content_bytes = await response.read()
                
                try:
                    content = content_bytes.decode(encoding)
                except UnicodeDecodeError:
                    # –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                    for fallback_encoding in ['utf-8', 'cp1251', 'iso-8859-1']:
                        try:
                            content = content_bytes.decode(fallback_encoding)
                            encoding = fallback_encoding
                            break
                        except UnicodeDecodeError:
                            continue
                    else:
                        logger.error(f"Failed to decode content for {url}")
                        return None
                
                return {
                    "content": content,
                    "encoding": encoding,
                    "response_headers": dict(response.headers),
                    "status_code": response.status,
                    "url": str(response.url)  # –§–∏–Ω–∞–ª—å–Ω—ã–π URL –ø–æ—Å–ª–µ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–≤
                }
                
        except asyncio.TimeoutError:
            logger.error(f"Timeout fetching {url}")
            return None
        except Exception as e:
            logger.error(f"Error fetching {url}: {e}")
            return None
    
    def _detect_encoding(self, response: aiohttp.ClientResponse, default: str = "utf-8") -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∫—É –æ—Ç–≤–µ—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ Content-Type
        content_type = response.headers.get('content-type', '').lower()
        if 'charset=' in content_type:
            try:
                charset = content_type.split('charset=')[1].split(';')[0].strip()
                return charset
            except:
                pass
        
        return default
    
    async def _parse_html_content(
        self, 
        url: str, 
        html_content: str, 
        encoding: str,
        site_config: SiteConfig,
        response_headers: Dict
    ) -> Optional[ScrapedDocument]:
        """–ü–∞—Ä—Å–∏—Ç HTML –∫–æ–Ω—Ç–µ–Ω—Ç"""
        try:
            from bs4 import BeautifulSoup
            
            # –°–æ–∑–¥–∞–µ–º soup –æ–±—ä–µ–∫—Ç
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            self._remove_unwanted_elements(soup, site_config.exclude_selectors)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = self._extract_title(soup, site_config.title_selectors, url)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content = self._extract_content(soup, site_config.content_selectors)
            
            if not content or len(content.strip()) < 100:
                logger.warning(f"Insufficient content extracted from {url}")
                return None
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = self._create_metadata(url, response_headers, encoding, soup)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            category = self._categorize_by_domain(urlparse(url).netloc)
            
            return ScrapedDocument(
                url=url,
                title=title,
                content=content,
                metadata=metadata,
                category=category
            )
            
        except Exception as e:
            logger.error(f"Error parsing HTML for {url}: {e}")
            return None
    
    def _remove_unwanted_elements(self, soup, exclude_selectors: str):
        """–£–¥–∞–ª—è–µ—Ç –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏–∑ soup"""
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        standard_removes = [
            "script", "style", "noscript", "iframe", 
            ".advertisement", ".ads", ".social-media",
            ".cookie-notice", ".popup", ".modal"
        ]
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π —Å–∞–π—Ç–∞
        all_selectors = standard_removes + exclude_selectors.split(", ")
        
        for selector in all_selectors:
            selector = selector.strip()
            if selector:
                for element in soup.select(selector):
                    element.decompose()
    
    def _extract_title(self, soup, title_selectors: str, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        for selector in title_selectors.split(", "):
            selector = selector.strip()
            if selector:
                element = soup.select_one(selector)
                if element and element.get_text(strip=True):
                    title = element.get_text(strip=True)
                    # –û—á–∏—â–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    title = re.sub(r'\s+', ' ', title)
                    if len(title) > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                        return title
        
        # Fallback - –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ URL
        try:
            domain = urlparse(url).netloc
            return f"–î–æ–∫—É–º–µ–Ω—Ç —Å {domain}"
        except:
            return "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç"
    
    def _extract_content(self, soup, content_selectors: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        content_parts = []
        
        # –ü—Ä–æ–±—É–µ–º —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –ø–æ –ø–æ—Ä—è–¥–∫—É
        for selector in content_selectors.split(", "):
            selector = selector.strip()
            if selector:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text()
                    if text and len(text.strip()) > 50:
                        content_parts.append(text)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º, –±–µ—Ä–µ–º –≤–µ—Å—å body
        if not content_parts:
            body = soup.find('body')
            if body:
                content_parts.append(body.get_text())
        
        if not content_parts:
            return ""
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –æ—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
        combined_content = "\n\n".join(content_parts)
        return self._clean_extracted_text(combined_content)
    
    def _clean_extracted_text(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç"""
        if not text:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'\.{3,}', '...', text)
        text = re.sub(r'-{3,}', '---', text)
        text = re.sub(r'={3,}', '===', text)
        
        # –£–±–∏—Ä–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ (–≤–µ—Ä–æ—è—Ç–Ω–æ, –º–µ–Ω—é –∏–ª–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—è)
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if len(line) > 15:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å—Ç—Ä–æ–∫–∏
                cleaned_lines.append(line)
        
        result = '\n'.join(cleaned_lines).strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
        if len(result) < 100:
            return ""
        
        return result
    
    def _create_metadata(self, url: str, response_headers: Dict, encoding: str, soup) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        metadata = {
            "scraped_at": time.time(),
            "url": url,
            "domain": urlparse(url).netloc,
            "encoding": encoding,
            "real_scraping": True,
            "scraper_version": "2.0"
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        if response_headers:
            metadata.update({
                "content_type": response_headers.get('content-type', ''),
                "server": response_headers.get('server', ''),
                "last_modified": response_headers.get('last-modified', ''),
                "etag": response_headers.get('etag', '')
            })
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞-—Ç–µ–≥–∏
        if soup:
            meta_description = soup.find('meta', attrs={'name': 'description'})
            if meta_description:
                metadata["description"] = meta_description.get('content', '')[:500]
            
            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
            if meta_keywords:
                metadata["keywords"] = meta_keywords.get('content', '')[:500]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫
            html_tag = soup.find('html')
            if html_tag and html_tag.get('lang'):
                metadata["language"] = html_tag.get('lang')
        
        return metadata
    
    def _categorize_by_domain(self, domain: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –¥–æ–º–µ–Ω—É"""
        domain_lower = domain.lower()
        
        # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ —Å–∞–π—Ç—ã
        if any(ua_domain in domain_lower for ua_domain in [
            "zakon.rada.gov.ua", "rada.gov.ua", "court.gov.ua", 
            "minjust.gov.ua", "ccu.gov.ua", "npu.gov.ua"
        ]):
            return "ukraine_legal"
        
        # –ò—Ä–ª–∞–Ω–¥—Å–∫–∏–µ —Å–∞–π—Ç—ã
        if any(ie_domain in domain_lower for ie_domain in [
            "irishstatutebook.ie", "courts.ie", "citizensinformation.ie",
            "justice.ie", "oireachtas.ie", "gov.ie"
        ]):
            return "ireland_legal"
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤ –¥–æ–º–µ–Ω–µ
        if any(keyword in domain_lower for keyword in ["law", "legal", "court", "justice"]):
            return "legislation"
        elif any(keyword in domain_lower for keyword in ["zakon", "pravo", "sud"]):
            return "legislation"
        elif "court" in domain_lower or "sud" in domain_lower:
            return "jurisprudence"
        elif any(keyword in domain_lower for keyword in ["gov", "government", "ministry"]):
            return "government"
        elif any(keyword in domain_lower for keyword in ["citizen", "immigration", "rights"]):
            return "civil_rights"
        else:
            return "scraped"
    
    async def scrape_multiple_urls(
        self, 
        urls: List[str], 
        delay: float = 1.0,
        max_concurrent: int = 3
    ) -> List[Optional[ScrapedDocument]]:
        """
        –ü–∞—Ä—Å–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ URL —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        
        Args:
            urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
            max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ ScrapedDocument –æ–±—ä–µ–∫—Ç–æ–≤
        """
        logger.info(f"üöÄ Starting bulk scrape of {len(urls)} URLs (max_concurrent={max_concurrent})")
        
        semaphore = asyncio.Semaphore(max_concurrent)
        results = []
        
        async def scrape_with_delay(url: str, index: int) -> Optional[ScrapedDocument]:
            async with semaphore:
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if index > 0 and delay > 0:
                    await asyncio.sleep(delay)
                
                return await self.scrape_legal_site(url)
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö URL
        tasks = [
            scrape_with_delay(url.strip(), i) 
            for i, url in enumerate(urls) 
            if url.strip()
        ]
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Error processing URL {urls[i]}: {result}")
                processed_results.append(None)
            else:
                processed_results.append(result)
        
        successful = len([r for r in processed_results if r is not None])
        logger.info(f"‚úÖ Bulk scrape completed: {successful}/{len(urls)} successful")
        
        return processed_results
    
    async def _create_demo_document(self, url: str) -> ScrapedDocument:
        """–°–æ–∑–¥–∞–µ—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"""
        demo_content = f"""
–î–ï–ú–û: –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç —Å {url}

‚ö†Ô∏è –≠—Ç–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç. –†–µ–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –æ–¥–Ω–æ–π –∏–∑ –ø—Ä–∏—á–∏–Ω:
- –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ aiohttp/beautifulsoup4
- –°–∞–π—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –¥–æ—Å—Ç—É–ø
- –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç—å—é
- –ù–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã

üìã –î–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
pip install aiohttp beautifulsoup4

üìÑ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–µ–º–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:
–≠—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö –∏ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞—Ö.
–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞, –ø—Ä–∞–≤–∞ –∏ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –≥—Ä–∞–∂–¥–∞–Ω,
–ø—Ä–æ—Ü–µ–¥—É—Ä—ã –æ–±—Ä–∞—â–µ–Ω–∏—è –≤ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã –∏ —Å—É–¥–µ–±–Ω—É—é —Å–∏—Å—Ç–µ–º—É.

üîç –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã:
1. –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∞ –≥—Ä–∞–∂–¥–∞–Ω
2. –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–µ –∏ —É–≥–æ–ª–æ–≤–Ω–æ–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ  
3. –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã
4. –°—É–¥–µ–±–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∏ –ø—Ä–æ—Ü–µ—Å—Å—ã
5. –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –≥–∞—Ä–∞–Ω—Ç–∏–∏ –∏ –ª—å–≥–æ—Ç—ã

üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –¥–∞–Ω–Ω—ã–µ:
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ: 1,247
- –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è –¥–µ–ª: 30-45 –¥–Ω–µ–π
- –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π: 67%

üìû –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–±—Ä–∞—â–∞–π—Ç–µ—Å—å –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ
–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã –∏–ª–∏ –∫ –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —é—Ä–∏—Å—Ç–∞–º.

üïí –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {time.strftime('%Y-%m-%d %H:%M:%S')}
üåê URL: {url}
‚ö° –°—Ç–∞—Ç—É—Å: –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ä–µ–∂–∏–º

üìù –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –í —Ä–µ–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ –∑–¥–µ—Å—å –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã,
–∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–µ —Å –ø–æ–º–æ—â—å—é –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML.
""".strip()
        
        domain = urlparse(url).netloc
        
        return ScrapedDocument(
            url=url,
            title=f"–î–ï–ú–û: –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç —Å {domain}",
            content=demo_content,
            metadata={
                "scraped_at": time.time(),
                "status": "demo",
                "real_scraping": False,
                "demo_version": "2.0",
                "url": url,
                "domain": domain,
                "reason": "Dependencies missing or scraping failed"
            },
            category=self._categorize_by_domain(domain)
        )
    
    def _update_response_time(self, elapsed: float):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞"""
        if self.stats["total_requests"] > 0:
            current_avg = self.stats["average_response_time"]
            total_requests = self.stats["total_requests"]
            self.stats["average_response_time"] = (
                (current_avg * (total_requests - 1) + elapsed) / total_requests
            )
    
    def get_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã —Å–∫—Ä–∞–ø–µ—Ä–∞"""
        success_rate = 0
        if self.stats["total_requests"] > 0:
            success_rate = (self.stats["successful_scrapes"] / self.stats["total_requests"]) * 100
        
        return {
            **self.stats,
            "success_rate": round(success_rate, 2),
            "demo_mode": self.demo_mode,
            "configured_sites": len(self.legal_sites_config),
            "supported_domains": list(self.legal_sites_config.keys())
        }
    
    def reset_stats(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        self.stats = {
            "total_requests": 0,
            "successful_scrapes": 0,
            "failed_scrapes": 0,
            "demo_responses": 0,
            "average_response_time": 0.0
        }
    
    async def validate_url(self, url: str) -> Dict:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç URL –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º"""
        validation_result = {
            "url": url,
            "valid": False,
            "reachable": False,
            "issues": [],
            "warnings": [],
            "recommendations": [],
            "site_config_available": False
        }
        
        try:
            # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è URL
            parsed = urlparse(url)
            
            if not parsed.scheme:
                validation_result["issues"].append("Missing URL scheme (http/https)")
            elif parsed.scheme not in ["http", "https"]:
                validation_result["issues"].append(f"Unsupported scheme: {parsed.scheme}")
            
            if not parsed.netloc:
                validation_result["issues"].append("Missing domain name")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Å–∞–π—Ç–∞
            domain = parsed.netloc.lower()
            if domain in self.legal_sites_config:
                validation_result["site_config_available"] = True
                validation_result["recommendations"].append("Site-specific configuration available")
            
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
            if parsed.netloc in ["localhost", "127.0.0.1", "0.0.0.0"]:
                validation_result["warnings"].append("Local URLs may not be accessible")
            
            if parsed.netloc.endswith(".local"):
                validation_result["warnings"].append("Local domain detected")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å URL (–µ—Å–ª–∏ –Ω–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫)
            if not validation_result["issues"]:
                validation_result["valid"] = True
                
                try:
                    await self._ensure_session()
                    async with self.session.head(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                        validation_result["reachable"] = response.status == 200
                        if response.status != 200:
                            validation_result["warnings"].append(f"HTTP {response.status} status code")
                except Exception as e:
                    validation_result["warnings"].append(f"Connectivity issue: {str(e)}")
            
            return validation_result
            
        except Exception as e:
            validation_result["issues"].append(f"Validation error: {str(e)}")
            return validation_result
    
    async def get_site_info(self, url: str) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∞–π—Ç–µ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        try:
            await self._ensure_session()
            
            async with self.session.head(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                headers = dict(response.headers)
                
                return {
                    "url": url,
                    "status_code": response.status,
                    "server": headers.get('server', 'Unknown'),
                    "content_type": headers.get('content-type', 'Unknown'),
                    "content_length": headers.get('content-length', 'Unknown'),
                    "last_modified": headers.get('last-modified', 'Unknown'),
                    "reachable": response.status == 200,
                    "redirect_url": str(response.url) if str(response.url) != url else None
                }
                
        except Exception as e:
            return {
                "url": url,
                "error": str(e),
                "reachable": False
            }

# –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å–∞–π—Ç–æ–≤
UKRAINE_LEGAL_URLS = [
    "https://zakon.rada.gov.ua/laws/main",
    "https://court.gov.ua/",
    "https://minjust.gov.ua/",
    "https://ccu.gov.ua/",
    "https://npu.gov.ua/",
    "https://sfs.gov.ua/",
    "https://dpsu.gov.ua/"
]

IRELAND_LEGAL_URLS = [
    "https://www.irishstatutebook.ie/",
    "https://www.courts.ie/",
    "https://www.citizensinformation.ie/en/",
    "https://www.justice.ie/",
    "https://www.oireachtas.ie/",
    "https://www.gov.ie/en/"
]

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã

class ScrapingBatch:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç –ø–∞–∫–µ—Ç–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –∏ –æ—á–µ—Ä–µ–¥—è–º–∏"""
    
    def __init__(self, scraper: LegalSiteScraper):
        self.scraper = scraper
        self.queue = asyncio.Queue()
        self.results = {}
        self.active_tasks = set()
    
    async def add_url(self, url: str, priority: int = 1, custom_config: Dict = None):
        """–î–æ–±–∞–≤–ª—è–µ—Ç URL –≤ –æ—á–µ—Ä–µ–¥—å –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        await self.queue.put({
            "url": url,
            "priority": priority,
            "custom_config": custom_config,
            "added_at": time.time()
        })
    
    async def process_batch(self, max_workers: int = 3) -> Dict:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–∞–∫–µ—Ç URL"""
        workers = []
        
        for i in range(max_workers):
            worker = asyncio.create_task(self._worker(f"worker-{i}"))
            workers.append(worker)
            self.active_tasks.add(worker)
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
        await self.queue.join()
        
        # –û—Ç–º–µ–Ω—è–µ–º –≤–æ—Ä–∫–µ—Ä–æ–≤
        for worker in workers:
            worker.cancel()
        
        await asyncio.gather(*workers, return_exceptions=True)
        
        return self.results
    
    async def _worker(self, name: str):
        """–í–æ—Ä–∫–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ URL –∏–∑ –æ—á–µ—Ä–µ–¥–∏"""
        while True:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∑–∞–¥–∞—á—É –∏–∑ –æ—á–µ—Ä–µ–¥–∏
                task = await self.queue.get()
                
                logger.info(f"üîß {name} processing: {task['url']}")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º URL
                result = await self.scraper.scrape_legal_site(
                    task["url"], 
                    task.get("custom_config")
                )
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                self.results[task["url"]] = {
                    "document": result,
                    "processed_at": time.time(),
                    "worker": name,
                    "success": result is not None
                }
                
                # –û—Ç–º–µ—á–∞–µ–º –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é
                self.queue.task_done()
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Worker {name} error: {e}")
                self.queue.task_done()

async def test_scraper():
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∫—Ä–∞–ø–µ—Ä–∞"""
    async with LegalSiteScraper() as scraper:
        test_urls = [
            "https://www.citizensinformation.ie/en/moving-country/irish-citizenship/",
            "https://www.example.com",  # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ fallback
            "https://httpbin.org/html"  # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        ]
        
        print("üß™ Testing Legal Site Scraper")
        print("=" * 50)
        
        for url in test_urls:
            print(f"\nüîç Testing: {url}")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
            validation = await scraper.validate_url(url)
            print(f"‚úÖ Valid: {validation['valid']}, Reachable: {validation['reachable']}")
            
            # –ü–∞—Ä—Å–∏–Ω–≥
            document = await scraper.scrape_legal_site(url)
            
            if document:
                print(f"üìÑ Title: {document.title}")
                print(f"üìä Content: {len(document.content)} chars")
                print(f"üè∑Ô∏è Category: {document.category}")
                print(f"üîç Real scraping: {document.metadata.get('real_scraping', False)}")
                print(f"üìù Preview: {document.content[:150]}...")
            else:
                print("‚ùå Failed to scrape")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = scraper.get_stats()
        print(f"\nüìä Scraper Statistics:")
        print(f"   Total requests: {stats['total_requests']}")
        print(f"   Success rate: {stats['success_rate']}%")
        print(f"   Average response time: {stats['average_response_time']:.2f}s")

# –≠–∫—Å–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
__all__ = [
    'LegalSiteScraper',
    'ScrapedDocument', 
    'SiteConfig',
    'ScrapingBatch',
    'UKRAINE_LEGAL_URLS',
    'IRELAND_LEGAL_URLS',
    'test_scraper'
]

if __name__ == "__main__":
    asyncio.run(test_scraper())